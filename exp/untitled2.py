# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19V6xKZJPQWfk3Zl81AT4FtOsMDlJQeOX
"""

!pip install transformers[sentencepiece]

import math
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

class PositionalEncoding(nn.Module):
  def __init__(self, emb_size, dropout, maxlen=5000):
    super(PositionalEncoding, self).__init__()
    den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)
    pos = torch.arange(0, maxlen).reshape(maxlen, 1)
    pos_embedding = torch.zeros((maxlen, emb_size))
    pos_embedding[:, 0::2] = torch.sin(pos * den)
    pos_embedding[:, 1::2] = torch.cos(pos * den)
    pos_embedding = pos_embedding.unsqueeze(-2)

    self.dropout = nn.Dropout(dropout)
    self.register_buffer('pos_embedding', pos_embedding)
  
  def forward(self, token_embedding):
    return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

class TokenEmbedding(nn.Module):
  def __init__(self, vocab_size, emb_size):
    super(TokenEmbedding, self).__init__()
    self.embedding = nn.Embedding(vocab_size, emb_size)
    self.emb_size = emb_size
  
  def forward(self, tokens):
    return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

class Seq2SeqTransformer(nn.Module):
  def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):
    super(Seq2SeqTransformer, self).__init__()
    self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)
    self.generator = nn.Linear(emb_size, tgt_vocab_size)
    self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
    self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
    self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

  def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):
    src_emb = self.positional_encoding(self.src_tok_emb(src))
    tgt_emb = self.positional_encoding(self.tgt_tok_emb(src))
    outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
    return self.generator(outs)
  
  def encode(self, src, src_mask):
    return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)
  
  def decode(self, tgt, memory, tgt_mask):
    return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)

kyoto_train_en = '/content/kyoto-train.en'
kyoto_train_ja = '/content/kyoto-train.ja'

kyoto_test_en = '/content/kyoto-test.en'
kyoto_test_ja = '/content/kyoto-test.ja'

with open(kyoto_train_en) as f:
  en_train_lines = f.readlines()

with open(kyoto_train_ja) as f:
  ja_train_lines = f.readlines()

with open(kyoto_test_en) as f:
  en_test_lines = f.readlines()

with open(kyoto_test_ja) as f:
  ja_test_lines = f.readlines()

lang = ["en", "ja"]

train_datasets = []
test_datasets = []

for i in range(len(en_train_lines)):
  line = [en_train_lines[i], ja_train_lines[i]]
  dictionary = dict(zip(lang, line))
  train_datasets.append(dictionary)

for i in range(len(en_test_lines)):
  line = [en_test_lines[i], ja_test_lines[i]]
  dictionary = dict(zip(lang, line))
  test_datasets.append(dictionary)

short_train_datasets = []
for i in range(50000):
  short_train_datasets.append(train_datasets[i])

train_datasets = short_train_datasets

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-tatoeba-en-ja", return_tensors="pt")

def preprocess_function(examples):
  inputs = [ex["en"] for ex in examples]
  targets = [ex["ja"] for ex in examples]
  model_inputs = tokenizer(inputs, max_length=128, truncation=True)

  with tokenizer.as_target_tokenizer():
    labels = tokenizer(targets, max_length=128, truncation=True)

  model_inputs["labels"] = labels["input_ids"]
  
  return model_inputs

model_train_inputs = preprocess_function(train_datasets)
model_test_inputs = preprocess_function(test_datasets)

def to_tensors(model_inputs, model_datasets):
  data_list = []

  data = list(model_inputs.data)

  for i in range(len(model_datasets)):
    nakami = [torch.tensor(model_inputs.data['input_ids'][i]), torch.tensor(model_inputs.data['attention_mask'][i]), torch.tensor(model_inputs.data['labels'][i])]
    dictionary = dict(zip(data, nakami))
    data_list.append(dictionary)

  model_inputs.data = data_list

  return model_inputs

model_train_inputs = to_tensors(model_train_inputs, train_datasets)
model_test_inputs = to_tensors(model_test_inputs, test_datasets)

SRC_VOCAB_SIZE = len(train_datasets)
TGT_VOCAB_SIZE = len(train_datasets)
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 128
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3

transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)

for p in transformer.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

transformer_2 = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-tatoeba-en-ja")