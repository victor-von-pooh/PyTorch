{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "en2jp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPW45lFS6r6D"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] seqeval sacrebleu huggingface_hub accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from huggingface_hub import notebook_login\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "hK6XTnNz_MWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#データセットの用意\n",
        "lang1, lang2 = \"en\", \"ja\" #要定義\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=lang1, lang2=lang2)\n",
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=8192)\n",
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
        "\n",
        "#トークナイザーの準備\n",
        "model_checkpoint = \"Helsinki-NLP/opus-tatoeba-en-ja\" #要定義\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
        "\n",
        "#前処理の関数定義\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[lang1] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[lang2] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "#前処理を一気に行う\n",
        "tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "1olO5uOC61XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    return {\"bleu\": result[\"score\"]}\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"marian-finetuned-kde4-en-to-ja\", #要定義\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.evaluate(max_length=128)\n",
        "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
      ],
      "metadata": {
        "id": "ruvXvQl00Ewx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}