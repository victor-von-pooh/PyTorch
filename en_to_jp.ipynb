{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "en2jp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JgewsHAYYVV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] seqeval sacrebleu huggingface_hub accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from huggingface_hub import get_full_repo_name, notebook_login, Repository\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, get_scheduler, pipeline, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "4rxqUjYGYnrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang1, lang2 = \"en\", \"ja\"\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=lang1, lang2=lang2)\n",
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=8192)\n",
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
      ],
      "metadata": {
        "id": "Htc6yRb0KUJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-tatoeba-en-ja\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "2-F_VHenKZ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [ex[lang1] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[lang2] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "t9Pu6DV4YrBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "r4HO9zTIKhY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "MsX32ZsrKkwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
        ")\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "C9zjbJI-KobZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "model_name = \"marian-finetuned-kde4-en-to-ja-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "\n",
        "output_dir = \"marian-finetuned-kde4-en-to-ja-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)\n",
        "\n",
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    return decoded_preds, decoded_labels"
      ],
      "metadata": {
        "id": "iB1xwj_mYubg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    model.eval()\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "            )\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        generated_tokens = accelerator.pad_across_processes(\n",
        "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(generated_tokens)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )\n",
        "\n",
        "model.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
      ],
      "metadata": {
        "id": "0w5Jp1jSLHMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Hoax0930/marian-finetuned-kde4-en-to-ja\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)"
      ],
      "metadata": {
        "id": "5cV9eoIGKKHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input('原文 : ')\n",
        "print(f'訳 : {translator(sentence)}')"
      ],
      "metadata": {
        "id": "4TipueaYMvjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}